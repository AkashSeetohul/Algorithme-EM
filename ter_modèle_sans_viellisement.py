# -*- coding: utf-8 -*-
"""TER - modèle sans viellisement.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ow23ftDVU9SOHP3YOflSK2xuRR7i0QL3
"""

import numpy as np
import math as math
from scipy.stats import expon
from scipy.stats import rv_discrete

class loi_melange_exponentiel:

  def __init__(self, alpha, lambda_):
    self.alpha = alpha
    self.lambda_ = lambda_
    # la classe contient aussi la distribution marginale des machines - chaque élément de l'array alpha représente le probabililité de chaque classe.
    self.distribution_Z = rv_discrete(a=0, b=len(alpha), values=(np.arange(len(alpha)),alpha))

  # methode pour renvoyer une array d'échantillon d'un taille donné
  def echantilloner(self, taille):
    # L'échantillon z déterminera la valeur de lambda_j utilisée, grâce à la présence de l'indicatrice dans chaque terme de l'expression
    z = self.distribution_Z.rvs(size=taille)
    print(z)

    x_echantillon = np.zeros(shape = taille)

    for indice in range(taille):
      realization_Z = z[indice]
      x_echantillon[indice] = expon.rvs(scale=1/(self.lambda_[realization_Z]), size=1) # une echantillon de X

    return x_echantillon

# on simule des données pour l'algorithme pour les vrais paramètres ci-dessus
alpha_vrai = [0.15, 0.25, 0.2, 0.1, 0.3]
lambda_vrai = [5, 7, 2, 9, 8]

# on génère des échantillons/observations
loi_melange = loi_melange_exponentiel(alpha=alpha_vrai, lambda_=lambda_vrai)
x_echantillon = loi_melange.echantilloner(taille=200)

# les paramètres initiales
alpha_estime = [0.10, 0.20, 0.3, 0.15, 0.25]
lambda_estime = [7, 5, 3, 10, 6]

# une fonction qui calcule f(x, alpha_j)
# Paramètres: une réalisation de X, alpha_j, et lambda_j
def f(alpha_j, lambda_j, x):
    return alpha_j * expon.pdf(x, scale= 1/lambda_j)

# On définit nos critère d'arrêt
max_iterations = 1200
tolerance = 0.00001

# initialization des variables pour la boucle while
nombre_iterations = 0
difference = 1000

# on initialize une boucle avec les conditions d'arrêt - tolerance et nombre maximale d'itérations
while (nombre_iterations < max_iterations) and (difference > tolerance):
  # arrays pour les prochains paramètres
  nouveau_alpha = np.zeros(len(alpha_estime))
  nouveau_lambda = np.zeros(len(lambda_estime))

  # on initialise la matrice H
  H = np.zeros((len(x_echantillon), len(alpha_estime)))

  # on calcule les valeurs de H_ij basée sur les formules du Section 6.1
  for i in range(len(x_echantillon)):
    for j in range(len(alpha_estime)):

        # on calcule f(x_i,alpha) pour chaque alpha - le somme de denom serait le denominateur de H_ij
        denom = np.zeros(len(alpha_estime))
        for indice in range(len(alpha_estime)):
          denom[indice] = f(alpha_j=alpha_estime[indice], lambda_j=lambda_estime[indice], x=x_echantillon[i])

        H[i,j] = f(alpha_j=alpha_estime[j], lambda_j=lambda_estime[j], x=x_echantillon[i]) / np.sum(denom)

  # on calcule les prochaines estimations basées sur les formules du Section ...
  for index in range(len(alpha_estime)):
    nouveau_alpha[index] = np.sum(H[:, index])/len(x_echantillon)
    nouveau_lambda[index] = np.sum(H[:, index])/np.dot(np.transpose(x_echantillon) ,H[:, index])

  # mise à jour des variables pour le critère d'arrêt
  nombre_iterations += 1
  difference = np.sqrt(math.dist(nouveau_alpha,alpha_estime) + math.dist(nouveau_lambda,lambda_estime))

  # mise à jour de nos parametres
  alpha_estime = nouveau_alpha
  lambda_estime = nouveau_lambda

  if nombre_iterations==250 or nombre_iterations==500 or nombre_iterations==750 or nombre_iterations==1000:
      print(nombre_iterations)
      print(alpha_estime)
      print(lambda_estime)